<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<title>Statistical Models: Theory and Applications</title>

<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<!--	<link href='http://fonts.googleapis.com/css?family=Fanwood+Text:400,400italic' rel='stylesheet' type='text/css'> -->
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<!--[if lt IE 9]>
 <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
 <![endif]-->
</head>
<body>
<div class="wrapper">
<header>
<h1><a href="index.html">Statistical Models: Theory and Applications</a> <h1><a><medium>Bin Yu and Rebecca Barter</medium></a></h1></h1>
<p></p>

<p class="view"><a href="https://github.com/rlbarter/Stat_Models_Book">View the Project on GitHub <small>rlbarter/Stat_Models_Book</small></a></p>




</header>










<section>  
<!-- add option for LaTeX> -->
<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
<script type="text/javascript">
LatexIT.add('p',true);
</script>

<h1>An introduction to causal inference</h1>

<p>Recall the egg-yolk study from the <a href="1-philosophy.html">how to be an applied statistician</a> section in which we discussed the many issues with the study and touched upon why the study didn&#39;t provide enough evidence to draw a causal conclusion. In this section, we describe the general principles for designing studies from which we could conceviebly draw causal conclusions. We will also introduce the Neyman-Rubin model for describing the causal inference framework.</p>

<h2>Observational studies vs randomized controlled experiments</h2>

<p>It generally considered that the gold-standard for drawing causal inferences is to conduct a randomized, controlled experiment. The idea begind this notion is as follows: suppose that, from your population of interest, you randomly select $N$ people. You then randomly split your sample in half, so that $N/2$ are in the &quot;treatment&quot; group and $N/2$ are in the &quot;control&quot; group. Then, on average, the treatment and control groups would be statistically equivalent. In particular, if we apply an intervention (a treatment) to the treatment group, but not to the control group, then the only difference between these two groups is the fact that the treatment group recieved the treatment. Thus, any difference observed after applying the treatment must be a causal effect of the treatment itself (rather than a reflection of some intial underlying difference between the two groups).</p>

<p>In contrast, an observational study in one in which the experimenter simply observes the differences between subjects who have placed themselves in the treatment group and those who have not (this is in contrast to a randomized, controlled experiment, where the experimenter decided, albeit randomly, who recieved the treatment and who did not). An example is if we wanted to estimate the causal effect of smoking on cancer. It would certainly be unethical to randomly assign half of our sample of people to smoke and the other half not to be. A much more reasonable approach would be to compare people who are already smokers to people who are not smokers and compare the proportion of people in each group who develop lung cancer. What is wrong with this approach? The primary issue is that there may be underlying differences (for example genetic differences) between people who choose to smoke and people who do not, and these issues may cause those who smoke to also develop lung cancer. In general, the principal problem with observational studies is that there may be underlying differences between the treatment and control group, other than the treatment itself, which influences the response. Such variables are called <em>confounders</em>. The question to ask thus becomes: how can we be sure that any differences we observe between the treatment and the control group are due to the treatment rather than the confounding variables?</p>

<h2>Why randomize?</h2>

<p>The most obvious argument for randomization is related to the issues discussed above with observatonal studies: to allow for balanced covariates (variables related to the response) between the treatment and control groups. However, there is a common misconception that randomization <em>ensures</em> balanced covariates. This is not necessarily true, and the likelihood of balanced covariates depends strongly on the randomization procedure undertaken. Consider, for example, the case where assignment to treatment is done by the flip of a coin: for each individual, if the coin flip shows heads then they are assigned to the treatment group, and if it shows tails they are assigned to the control group. Then, if there are $N$ individuals in the study, there are $2^N$ different possible ways to treatment assignment (each individual has two possible assignments: treatment or control). Moreover, several of these treatment assignment outcomes result in useless studies: it is entirely possible (though unlikely) that under this randomization procedure, everyone will be assigned to the treatment group. In which case it is impossible to make a causal inference about the effect of the treatment on the response (since you have no control group to compare to).</p>

<p>For another example, consider a study aimed at assessing the effectiveness of a type of heart surgery on 1,000 patients from a total of 12 different hospitals. Surely the outcome will vary from hospital to hospital. Thus even if the randomization is conducted such that half of the patients are in the treatment group and the other half are in the control group, there is the possibility that the treatment group will contain only patients from hospitals 1, 2, 4, 7, 8 and 12 and the control group contained only patients from hospitals 3, 5, 6, 9, 10 and 11. This mechanism has the property that we are never comparing patients from the same hospital, moreover, it is likely that each hospital is different, and thus that our treatment and control groups are not necessarily comparable. How could we conduct the randomization to increase the chance obtaining more comparable (more balanced) treatment and control groups? One way is to perform the randomization procedure individually <em>within</em> each hospital. This process is called <em>stratification</em>.</p>

<p>Thus it is certainly not the case that a randomization ensures that the study will be well-designed, but hopefully we can agree that randomization studies are more likely to lead to balanced covariates than observational studies. An analyst needs to think carefully about how to conduct the randomization to ensure that the control and treatment groups are equivalent. In the (rare) case that the covariates are perfectly balanced between the treatment and control groups, then to obtain an unbiased estimate the causal effect of the treatment, we could simply calculate:</p>

<p>(Average outcome of the treatment group) - (Average outcome of the control group)</p>

<p>In ideal situations, this is the only number we ever really need to look at. Why would we need to fit linear models or conduct hypothesis tests when the difference in average outcomes tells us everything we need to know. <FONT COLOR="red">Expand on this</FONT> </p>

<p>Another point in favour of randomization of treatment assignment is that it makes the randomness in the problem explicit. As we will discuss when we introduce the Neyman-Rubin causal inference model, the only randomness in these types of problems comes from the assignment to treatment (in particular, the measured outcome is non-random). This is strikingly different to the traditional approach in which the outcome of interest is a random variable and the traditional approaches such as hypothesis testing and confidence intervals rest on properly defined probability distributions or sample spaces. This allows for more transparancy in the problem.</p>

<h2>Can we infer causation from observational studies?</h2>

<p>As discussed above, using randomized experiments it&#39;s certainly possible to infer that a treatment <em>causes</em> a change in some outcome, however we haven&#39;t yet discussed whether it&#39;s even possible to infer causation from observational studies. You may have heard the phrase <em>&quot;no causation without manipulation&quot;</em>, however we will consider an example (smoking) that represents a counterexample.</p>

<ul>
<li><p>matching - consider all confounders (never know if you&#39;ve observed <em>all</em> of them!)</p></li>
<li></li>
</ul>

<p>This is great news -- there are certainly many instances in which observational trials would be preferred to randomized trials.</p>

<h2>The Neyman-Rubin model</h2>

<p>Everything we have discussed in this chapter so far has a philosophical and non-rigorous flavour to it. The Neyman-Rubin framework represents the need to introduce rigour into the vague language of causal inference. The Neyman-Rubin model is named after Jerzy Neyman, who introduced the potential-outcomes framework for causality in 1923 and Donald Rubin who was a pioneer in extending the framework into the modern realm of causal inference in observational and randomized experiments.</p>

<h2>Notes</h2>

<p>There are a number of alternative approaches to the language of causal inference, the most prominent of which is that introduced by Judea Pearl.</p>

<p>In this chapter, we have just touched on the deep field of causal inference. For those who would like to delve further into the Neyman-Rubin approach to causal inference and learn more about the existing methods for conducting and analysing experimental and observational experiments, Imbens and Rubin&#39;s book, <a href="http://www.amazon.com/Causal-Inference-Statistics-Biomedical-Sciences/dp/0521885884">Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction</a>, is an excellent resource.</p>

</section>

<footer>

<p><small>Site powered by <a href="http://jekyllrb.com/">Jekyll, <a href="http://pandoc.org/">pandoc and <a href="http://yihui.name/knitr/">knitr</a>.</small></p>
</footer>


</div>
<script src="javascripts/scale.fix.js"></script>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-39541792-2', 'github.com');
ga('send', 'pageview');
</script>    
</body>
</html>
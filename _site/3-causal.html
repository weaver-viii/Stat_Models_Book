<!doctype html>
<html>
<head>

<!-- LaTeX -->

<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<title>The Elements of Data Science: A Perspective from Applied Statistics and Machine Learning</title>

<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<!--	<link href='http://fonts.googleapis.com/css?family=Fanwood+Text:400,400italic' rel='stylesheet' type='text/css'> -->
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<!--[if lt IE 9]>
 <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
 <![endif]-->
</head>
<body>
<div class="wrapper">
<header>
<h1><a href="index.html">The Elements of Data Science: A Perspective from Applied Statistics and Machine Learning</a> <h1><a><medium>Bin Yu and Rebecca Barter</medium></a></h1></h1>
<p></p>

<p class="view"><a href="https://github.com/rlbarter/Stat_Models_Book">View the Project on GitHub <small>rlbarter/Stat_Models_Book</small></a></p>




</header>










<section>  



<h2>The human need to infer causation</h2>

<p>Many of you have no doubt heard the much lauded phrase &quot;correlation does not imply causation&quot;. It is a well-known fact that although we continue to wag our fingers and utter the phrase, those less statistically informed (such as the media) continue to mistake correlation for causation. It is not hard to see why this fallacy is so common: there exists a fundamental human desire to infer causation as a result of our innate need to understand and explain the world in which we live. Our desire to draw causal conclusions begins from a young age; as a child you learn that you feel pain when you touch something hot. It is obvious that we don&#39;t understand the mechanism of why we felt pain when we touched something hot, but we inferred that it was the physical <em>touching</em> of the object that <em>caused</em> the pain. </p>

<p>It is clear that our subconscious need to infer causation is an evolutionary advantage, since although we don&#39;t have the means to explain <em>how</em> the cause and effect occurred, most of the time, our intuition is correct. The problem arises when we continue to draw this need to infer causation into the complex realm of science. As an example, consider drug advertisements. Most drug advertisements feature extraordinarily happy people, bright colors, and a passing description of what the drug is for. The idea behind these advertisements is that the human mind automatically infers causation from the association between the drug and happy people: the aim is to make people believe that the drug <em>causes</em> happiness (which if you pay attention to the speedily spoken side-effects at the end of the ad, is clearly the opposite of what the drug causes). </p>

<h3>The egg-yolk study</h3>

<p>Let&#39;s consider a Canadian study by Spence et al. on <em>egg yolk consumption and cartoid plaque</em> which aimed to identify whether the atherosclerosis burden (as a marker of arterial damage) was related to regular consumption of egg-yolk. The results of the study were touted in the media using phrases such as &quot;A new study suggests eating egg yolks can accelerate heart disease almost as much as smoking&quot;, and &quot;No yolk: [is] eating the whole egg as dangerous as smoking?&quot;.</p>

<p>The study was based on an 2831 consecutive patients attending vascular prevention clinics at University Hospital were asked to fill out a lifestyle questionnaire at the time of referral to the vascular prevention clinic. From the questionnaire, the authors calculated for each patient pack-years of smoking (number of packs per day of cigarettes times the number of years of smoking) and egg-yolk years (number of egg yolks per week times the number of years consumed). For each patient, each plaque identified in the common, external and internal carotid artery on both sides was measured in a longitudinal view, in the plane in which it was biggest. The perimeter of each plaque was measured by tracing a cursor on a computer screen. Of the 2831 patients with data on egg yolk consumption, 1231 consented to use the data and had data on pack-years of smoking and carotid total plaque area. Data range checks were performed and data entry errors such as decimal errors were identified by scatterplots of age against each continuous variable. The author&#39;s interpretation of the results in their study is that the &quot;data suggests a strong association between egg consumption and carotid plaque burden&quot;, and they conclude that their &quot;findings suggest that regular consumption of egg yolk should be avoided by persons at risk of cardiovascular disease&quot;.</p>

<p>Can you see any issues with this study? Are the conclusions drawn reasonable?</p>

<p>There are in fact a number of issues:</p>

<ol>
<li><p>The study is only considering people who had been referred to a particular vascular prevention clinic, yet the conclusion is making claims about the general population.</p></li>
<li><p>The study is observational AND they have no control group for comparison.</p></li>
<li><p>Do the measurements of plaque area seem reasonable? To me they seem very crude and are potentially a large source of error. This was not discussed at all in the media reports of the study.</p></li>
<li><p>&quot;All patients with complete data for each analysis were included&quot;: there is no explanation of how many patients had incomplete data, nor potential reasons for this. For example, what if people who did not consume egg-yolks or smoke simply chose not to fill out the questionnaire.</p></li>
<li><p>Less than half of the original study population were actually used in the study (either as a result of lack of consent or missing data) -- we are offered no information on how those who were used in the study differ from those who were not.</p></li>
<li><p>They jumped to causation in their conclusion by suggesting that &quot;regular consumption of egg yolk should be avoided by persons at risk of cardiovascular disease&quot;, which implies that consumption of egg yolk <em>causes</em> heart disease.</p></li>
</ol>

<p><FONT COLOR="red">Need to fill this out with a lot more information: make it clear why each point listed above is an issue.</FONT></p>

<h3>Quality over quantity: is a large sample size always better?</h3>

<p>Compare the relatively large egg-yolk study with the NZ filmmaker who put himself on a calorie controlled diet, but ensured that all of the calories came from high-fat junk food. Within 8 weeks he had gained an enormous amount of weight. This is a study with only one person (an incredibly small sample size), but which study would you believe more? Clearly sample size is not the only important thing; having a good design is <em>much</em> more important (quality over quantity).</p>

<h1>An introduction to causal inference</h1>

<p>In the egg-yolk study described above, we discussed the many issues with the study and touched upon why the study didn&#39;t provide enough evidence to draw a causal conclusion. We now move on to describe the general principles for designing studies from which we could conceivably draw causal conclusions. We will also introduce the Neyman-Rubin model for describing the causal inference framework.</p>

<h2>Observational studies vs randomized controlled experiments</h2>

<p>It generally considered that the gold-standard for drawing causal inferences is to conduct a randomized, controlled experiment. The idea behind this notion is as follows: suppose that, from your population of interest, you randomly select $N$ people. You then randomly split your sample in half, so that $N/2$ are in the &quot;treatment&quot; group and $N/2$ are in the &quot;control&quot; group. Then, on average, the treatment and control groups would be statistically equivalent. In particular, if we apply an intervention (a treatment) to the treatment group, but not to the control group, then the only difference between these two groups is the fact that the treatment group received the treatment. Thus, any difference observed after applying the treatment must be a causal effect of the treatment itself (rather than a reflection of some initial underlying difference between the two groups).</p>

<p>In contrast, an observational study in one in which the experimenter simply observes the differences between subjects who have placed themselves in the treatment group and those who have not (this is in contrast to a randomized, controlled experiment, where the experimenter decided, albeit randomly, who received the treatment and who did not). An example is if we wanted to estimate the causal effect of smoking on cancer. It would certainly be unethical to randomly assign half of our sample of people to smoke and the other half not to be. A much more reasonable approach would be to compare people who are already smokers to people who are not smokers and compare the proportion of people in each group who develop lung cancer. What is wrong with this approach? The primary issue is that there may be underlying differences (for example genetic differences) between people who choose to smoke and people who do not, and these issues may cause those who smoke to also develop lung cancer. In general, the principal problem with observational studies is that there may be underlying differences between the treatment and control group, other than the treatment itself, which influences the response. Such variables are called <em>confounders</em>. The question to ask thus becomes: how can we be sure that any differences we observe between the treatment and the control group are due to the treatment rather than the confounding variables?</p>

<h2>Why randomize?</h2>

<p>The most obvious argument for randomization is related to the issues discussed above with observational studies: to allow for balanced covariates (variables related to the response) between the treatment and control groups. However, there is a common misconception that randomization <em>ensures</em> balanced covariates. This is not necessarily true, and the likelihood of balanced covariates depends strongly on the randomization procedure undertaken. Consider, for example, the case where assignment to treatment is done by the flip of a coin: for each individual, if the coin flip shows heads then they are assigned to the treatment group, and if it shows tails they are assigned to the control group. Then, if there are $N$ individuals in the study, there are $2^N$ different possible ways to treatment assignment (each individual has two possible assignments: treatment or control). Moreover, several of these treatment assignment outcomes result in useless studies: it is entirely possible (though unlikely) that under this randomization procedure, everyone will be assigned to the treatment group. In which case it is impossible to make a causal inference about the effect of the treatment on the response (since you have no control group to compare to).</p>

<p>For another example, consider a study aimed at assessing the effectiveness of a type of heart surgery on 1,000 patients from a total of 12 different hospitals. Surely the outcome will vary from hospital to hospital. Thus even if the randomization is conducted such that half of the patients are in the treatment group and the other half are in the control group, there is the possibility that the treatment group will contain only patients from hospitals 1, 2, 4, 7, 8 and 12 and the control group contained only patients from hospitals 3, 5, 6, 9, 10 and 11. This mechanism has the property that we are never comparing patients from the same hospital, moreover, it is likely that each hospital is different, and thus that our treatment and control groups are not necessarily comparable. How could we conduct the randomization to increase the chance obtaining more comparable (more balanced) treatment and control groups? One way is to perform the randomization procedure individually <em>within</em> each hospital. This process is called <em>stratification</em>.</p>

<p>Thus it is certainly not the case that a randomization ensures that the study will be well-designed, but hopefully we can agree that randomization studies are more likely to lead to balanced covariates than observational studies. An analyst needs to think carefully about how to conduct the randomization to ensure that the control and treatment groups are equivalent. In the (rare) case that the covariates are perfectly balanced between the treatment and control groups, then to obtain an unbiased estimate the causal effect of the treatment, we could simply calculate:</p>

<p>(Average outcome of the treatment group) - (Average outcome of the control group)</p>

<p>In ideal situations, this is the only number we ever really need to look at. Why would we need to fit linear models or conduct hypothesis tests when the difference in average outcomes tells us everything we need to know. <FONT COLOR="red">Expand on this</FONT> </p>

<p>Another point in favor of randomization of treatment assignment is that it makes the randomness in the problem explicit. More specifically, we know exactly how the randomness was created and it is easily replicable. As we will discuss when we introduce the Neyman-Rubin causal inference model, the only randomness in these types of problems comes from the assignment to treatment (in particular, the measured outcome is non-random). This is strikingly different to the traditional approach in which the outcome of interest is a random variable and the traditional approaches such as hypothesis testing and confidence intervals rest on properly defined probability distributions or sample spaces. This allows for more transparency in the problem.</p>

<p>Randomization is certainly the gold-standard in inferring causation (although it is important not to accept a study as perfect just because the experiment was randomized. See the fruitfly experiment as an example).</p>

<p>Although the first people to truly emphasize the importance of randomization was R.A Fisher did so for experimental reasons. <FONT COLOR="red">a history of randomization</FONT>. These days, randomization has many uses beyond just experimental design. The modern approaches involve many randomized algorithms such as: compressed sensing, randomized algorithms for big data, Shannon&#39;s randomized channel code, random projections, non-parametric testing, etc.</p>

<h2>Can we infer causation from observational studies?</h2>

<p>As discussed above, using randomized experiments it&#39;s certainly possible to infer that a treatment <em>causes</em> a change in some outcome, however we haven&#39;t yet discussed whether it&#39;s even possible to infer causation from observational studies. You may have heard the phrase <em>&quot;no causation without manipulation&quot;</em>, however we will consider an example (smoking) that represents a counterexample.</p>

<ul>
<li><p>matching - consider all confounders (never know if you&#39;ve observed <em>all</em> of them!)</p></li>
<li></li>
</ul>

<p>This is great news -- there are certainly many instances in which observational trials would be preferred to randomized trials.</p>

<h2>The Neyman-Rubin model</h2>

<p>Everything we have discussed in this chapter so far has a philosophical and non-rigorous flavor to it. The Neyman-Rubin framework allows us to introduce rigor into the vague language of causal inference. The Neyman-Rubin model is named after Jerzy Neyman, who introduced the potential-outcomes framework for causality in 1923, and Donald Rubin who is a pioneer in extending this framework into the modern of causal inference in observational and randomized experiments.</p>

<p>The model is rooted in the notion that each individual in a two-arm study (control and treatment) has two <em>potential outcomes</em>. To make our descriptions transparent, we will embed our description of the Neyman-Rubin model within an example: suppose that we are interested in testing the effectiveness of a new drug on reducing the incidence of monthly headaches (our response). The patients assigned to the control group are not provided with the drug (perhaps they are provided with a placebo to make the study blinded, but we won&#39;t worry about this now), while the patients in the treatment group are provided with the drug and are told to take it daily. Each patient in each group then reports the number of headaches they experience in a month.</p>

<p>Let&#39;s think hypothetically about an individual patient (patient $i$). Suppose that when patient $i$ is assigned to the control group, they report experiencing 10 headaches. The notation we use is $Y_{i0} = 10$, where the 0 index corresponds to &quot;control&quot;. Alternatively, when patient $i$ is assigned to the treatment group, they report experiencing 8 headaches, and we write $Y_{i1} = 8$, where the 1 index corresponds to &quot;treatment&quot;. In reality, however, we can <em>only observe one</em> of these outcomes, since patient $i$ will be assigned <em>either</em> to the control group or to the treatment group, <em>but not both</em>. Hence we call $Y_{i0}$ and $Y_{i1}$ the <em>potential outcomes</em> for patient $i$. It is crucial to realize that both $Y_{i0}$ and $Y_{i1}$ are <em>fixed</em> values (i.e. they are not random!). The <em>observed</em> outcome (the outcome that we actually observe) for patient $i$ can be written as</p>

<p>$$Y_i = (1 - T_i) Y_{i0} + T_i Y_{i1}$$</p>

<p>where $T_i = 1$ if patient $i$ is assigned to the treatment group and $T_i = 0$ if patient $i$ is assigned to the control (recall that only one of these will occur). Note that if patient $i$ is assigned to the treatment group, we have</p>

<p>$$Y_i = 0 \times Y_{i0} + 1 \times Y_{i1} = Y_{i1}$$</p>

<p>whereas if patient $i$ is assigned to the control group, we have</p>

<p>$$Y_i = 1 \times Y_{i0} + 0 \times Y_{i1} = Y_{i0}$$</p>

<p>That is, we observe the potential outcome corresponding to which group patient $i$ is assigned.</p>

<h3>Where is the randomness in the Neyman-Rubin model?</h3>

<p>When presented with a new formulation or model, it is good practice to ask &quot;where does the randomness come from in this model?&quot;. In essence, we are asking &quot;what assumptions are we making about the randomness by using this model?&quot;.</p>

<p>Recall that the potential outcomes, $Y_{i0}$ and $Y_{i1}$, are considered <em>fixed</em> (non-random) values. The observed outcome, $Y_i$, however, is considered to be random. But given that $Y_i$ is just a linear combination of the potential outcomes, where does the randomness come from? The answer is that the randomness comes from the treatment assignment variable, $T_i$. That is, under this formulation, the only reason that we consider the observed outcome to be random is because the decision to place the patient in the treatment group or control group was random.</p>

<p>Thus the key difference between the Neyman-Rubin formulation and the traditional formulation is that in the Neyman-Rubin formulation a subject&#39;s outcome under each treatment is considered fixed, but the outcome that we observe is random (since it is random which treatment group the subject will be assigned to). In the traditional formulation, the response is considered to be random itself, but it is usually not explained where the randomness comes from. That is, the Neyman-Rubin model makes the randomness explicit!</p>

<p><FONT COLOR="red">I need to explain what I mean by the &quot;traditional formulation&quot;</FONT></p>

<p>Note that in the typical setting, we consider the <a href="1-philosophy.html#random">randomness</a> to come from the sampling procedure where we draw our sample from the population. Although this randomness also exists when we use the Neyman-Rubin model, we are not using this source of randomness for our analysis. In the Neyman-Rubin model, the primary source of randomness comes from the random assignment to the treatment or control group.</p>

<h2>Estimating the causal effect: an example</h2>

<p>Let&#39;s consider an example: suppose that you were the inventor of a new painkiller, and you needed to prove to the FDA that the painkiller was effective. How would you do this? Ideally, you would be able to perform a randomized trial Suppose that you had access to patients who visited a certain medical clinic over a period of a year. Then for each new patient who consented to participate in the trial, and whose primary reason for visiting the clinic was pain, you could conduct a random &quot;coin toss&quot; to decide if the doctor will prescribe the new painkiller to the patient or not. Note that in general the doctor cannot decide themselves whether or not to prescribe the painkiller, since this will introduce personal selection bias into the experiment, rather, the doctor must follow the decision made by an automated &quot;randomized&quot; process at the trial center. If the doctor is instructed to assign the patient to the treatment group, then they will prescribe the patient with the new painkiller, otherwise they will prescribe the patient to follow existing pain reliving paths. Over the duration of the trial, each enrolled patient must self-report their pain level on a scale of 1 to 10 (regardless of whether or not they received the new painkiller), and send their reports to the trial center. We are interested in identifying whether or not there is a difference in average headache severity between those who received the new painkiller and those who did not. To place this in the Neyman-Rubin framework, for patient $i$, we observe the average headache severity $Y_{i}$ which will be one of the two possible potential outcomes:</p>

<p>$$Y_{i} = \begin{cases} Y_{i0} &amp; \text{ if patient $i$ is assigned to the control group} \\ 
Y_{i1} &amp; \text{ if patient $i$ is assigned to the treatment group} \end{cases}$$ </p>

<p>That is, if patient $i$ is assigned to receive the new painkiller, we will observe the outcome $Y_{i1}$ (for example $Y_{i1} = 4$ implies that if the patient is assigned to take the new painkiller, they will have an average headache severity of 4), and we will observe the outcome $Y_{i0}$ otherwise (for example $Y_{i0} = 6$ implies that if the patient is not assigned to take the new painkiller, they will have an average headache severity of 6). Clearly we can only observe $Y_i = Y_{i0}$ or $Y_i = Y_{i1}$ based on treatment assignment. As discussed above, this can be re-written as</p>

<p>$$Y_i = (1 - T_i) Y_{i0} + T_i T_{i1}$$</p>

<p>where $T_i = 1$ if patient $i$ is assigned to the treatment group and $T_i = 0$ otherwise.</p>

<p>We are interested in identifying the effect of the treatment (being prescribed the placebo) on the outcome. Ideally, we would like to measure the difference between the outcome when assigned to treatment and when assigned to control for each person, i.e. we would like to observe $Y_{i1} - Y_{i0}$ (the difference in average headache intensity for patient $i$ for when they receive the painkiller and when they don&#39;t), and take the average over all patients enrolled in the study. This is called the sample average treatment effect (SATE):</p>

<p>$$SATE = \frac{1}{n}\sum_{i=1}^n (Y_{i1} - Y_{i0})$$</p>

<p>and it estimates the treatment effect (the effect of prescribing the painkiller). However, the SATE is not observable since for an individual we observe either $Y_{i1}$ OR $Y_{i0}$, not both (thus we certainly cannot calculate the difference $Y_{i1} - Y_{i0}$. An unbiased estimate of the SATE, however, can be given by the obvious estimator</p>

<p>$$\frac{1}{n_1} \sum_{i:T_i = 1} Y_{i} - \frac{1}{n_0} \sum_{i:T_i = 0} Y_{i}$$</p>

<p>whereby we are taking the average of the outcomes observed for those in the treatment group and subtracting the average of the outcomes observed for those in the control group. </p>

<h2>Hypothesis testing for treatment effect</h2>

<p>The notion of identifying a treatment effect can also be phrased as a hypothesis testing problem. Fisher posed this problem as testing the null hypothesis that</p>

<p>$$H_0: Y_{i1} = Y_{i0}$$</p>

<p>i.e. the treatment has <em>no effect on anyone</em>. This hypothesis, however, is not something that we can directly test since, once again, for each $i$ we only observe one of $Y_{i1}$ and $Y_{i0}$, and so cannot test the hypothesis that they will be the same. Alternatively, we can test the weaker hypothesis </p>

<p>$$H_0: \mu_1 = \mu_0$$</p>

<p>where $\mu_1$ is the population average response of those assigned to the treatment, and $\mu_0$ is the population average response of those assigned to control. This null hypothesis is equivalent to the claim that there is no difference between the average outcome of the control and the treatment groups. This hypothesis is in fact testable, and a suitable test statistic can be found in the SATE:</p>

<p>$$\tau = \overline{Y}_1^{obs} - \overline{Y}_0^{obs}$$</p>

<p>where $\overline{Y}_1^{obs} = \frac{1}{n_1} \sum_{i:T_i = 1}Y_i$ is the average outcome for the treatment group and $\overline{Y}_0^{obs} = \frac{1}{n_0} \sum_{i:T_i = 0}Y_i$ is the average outcome for the control group.</p>

<p>How do we obtain a notion of significance for this test statistic? Rather than estimating the variance of $\tau$, and using a distributional result to obtain a $p$-value, it is more common to conduct a permutation test to approximate the null distribution of $\tau$ and thus obtain a $p$-value. This involves</p>

<ol>
<li><p>Calculate the value of the test statistic, $\tau$</p></li>
<li><p>Randomly re-shuffle (permute) the treatment and assignment labels for each individual. For example if we had a (pathetically small) study involving 6 people, whose treatment assignments were $T = (1, 0, 0, 1, 0, 1)$ (i.e. individuals 1, 4 and 6 were assigned to treatment and individuals 2, 3 and 5 were assigned to control), we would randomly rearrange the order of the values in $T$ to get, for example, $T^* = (1, 1, 0, 1, 0, 0)$. </p></li>
<li><p>Re-evaluate the value of $\tau$ using the treatment assignment given by $T^*$: 
$$\tau^* = \frac{1}{n_1} \sum_{i:T^*_i = 1}Y_i - \frac{1}{n_0} \sum_{i:T^*_i = 0}Y_i$$</p></li>
<li><p>Repeat steps 2 and 3 many times (this will give an empirical estimate of the null distribution of $\tau$, whereby we are assuming that treatment and control are exchangeable: under the null hypothesis the treatment has no effect)</p></li>
<li><p>Calculate a $p$-value by calculating the proportion of $\tau^*$ values that are more &quot;extreme&quot; (larger in absolute value) than our true value, $\tau$.</p></li>
</ol>

<h2>Conclusion</h2>

<h2>Notes</h2>

<p>In this chapter, we have just touched on the deep field of causal inference. For those who would like to delve further into the Neyman-Rubin approach to causal inference and learn more about the existing methods for conducting and analyzing experimental and observational experiments, Imbens and Rubin&#39;s book, <a href="http://www.amazon.com/Causal-Inference-Statistics-Biomedical-Sciences/dp/0521885884">Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction</a>, is an excellent resource.</p>

<p>Further, it is worth noting that there are a number of alternative approaches to the language of causal inference, the most prominent of which is that of graphical models introduced by Judea Pearl.</p>



</section>

<footer>

<p><small>Site powered by <a href="http://jekyllrb.com/">Jekyll, <a href="http://pandoc.org/">pandoc and <a href="http://yihui.name/knitr/">knitr</a>.</small></p>
</footer>


</div>
<script src="javascripts/scale.fix.js"></script>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-39541792-2', 'github.com');
ga('send', 'pageview');
</script>    
</body>
</html>
<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<title>Statistical Models: Theory and Applications</title>

<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<!--	<link href='http://fonts.googleapis.com/css?family=Fanwood+Text:400,400italic' rel='stylesheet' type='text/css'> -->
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<!--[if lt IE 9]>
 <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
 <![endif]-->
</head>
<body>
<div class="wrapper">
<header>
<h1><a href="index.html">Statistical Models: Theory and Applications</a> <h1><a><medium>Bin Yu and Rebecca Barter</medium></a></h1></h1>
<p></p>

<p class="view"><a href="https://github.com/rlbarter/Stat_Models_Book">View the Project on GitHub <small>rlbarter/Stat_Models_Book</small></a></p>




</header>










<section>  
<!-- add option for LaTeX> -->
<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
<script type="text/javascript">
LatexIT.add('p',true);
</script>

<h1>How to be an applied statistician (this section is currently a smörgåsbord of wisdoms we wish to impart)</h1>

<p>In essence, this book is about how to be an applied statistician. More precisely, how to use the statistical methods you&#39;ve picked up (be it through your degree or by necessity) in practice and collaboration. Before embarking on a journey discussing how to implement your statistical skills in reality, we will first spend some time discussing the tools that are necessary for one to be a successful applied statistician. In this section, we focus not on using statistical methods, but on the importance of thinking critically, communicating effectively and understanding the philosophy behind statistics.</p>

<h2>The importance of communication</h2>

<p>There is an unfortunate presumption that statisticians (along with mathematicians, computer scientists, etc) are terrible at communication. If you have great ideas or are doing inspirational work but lack the skills to adequately communicate it (not just to other statisticians, but also potential collaborators), then you will find it hard to make much of an impact. Modern mathematical and statitsical education has equipped a generation that lacks awareness on the importance of being communicative, approachable and even likable, particularly for those who wish to work with non-statistician collaborators. </p>

<p>Not only do we devalue verbal communication skills, but we also underappreciate the importance of written communication skills. We focus so much on mastering the &quot;language of mathematics&quot; that we completely undermine our ability to portray our work to the outside world. Even our mathematical writing should have a narrative: words and imagery are just as important as equations, since your readers are much more likely to remember graphics and the overall story than theorems and formulae.</p>

<h2>The human need to infer causation</h2>

<p>Many of you have no doubt heard the much lauded phrase &quot;correlation does not imply causation&quot;. It is a well-known fact that although we continue to wag our fingers and utter the phrase, those less statistically informed (such as the media) continue to mistake correlation for causation. It is not hard to see why this fallacy is so common: there exists a fundamental human desire to infer causation as a result of our innate need to understand and explain the world in which we live. Our desire to draw causal conclusions begins from a young age; as a child you learn that you feel pain when you touch something hot. It is obvious that we don&#39;t understand the mechanism of why we felt pain when we touched something hot, but we inferred that it was the physical <em>touching</em> of the object that <em>caused</em> the pain. </p>

<p>It is clear that our subconscious need to infer causation is an evolutionary advantage, since although we don&#39;t have the means to explain <em>how</em> the cause and effect occured, most of the time, our intuition is correct. The problem arises when we continue to draw this need to infer causation into the complex realm of science. As an example, consider drug advertisements. Most drug advertisements feature extraordinarily happy people, bright colors, and a passing description of what the drug is for. The idea behind these advertisements is that the human mind automatically infers causation from the association between the drug and happy people: the aim is to make people believe that the drug <em>causes</em> happiness (which if you pay attention to the speedily spoken side-effects at the end of the ad, is clearly the opposite of what the drug causes). </p>

<h3>The egg-yolk study</h3>

<p>Let&#39;s consider a study conducted in 
Describe the egg yolk study and the reactions by the newspeople: &quot;Sounds great right?!&quot;. What are the problems? </p>

<h2>Sources of data</h2>

<p>A common problem among statisticians is finding good sources of data. The truth is that in most cases we are not the ones who actually go and collect the data, and as a result those without collaborators turn to databases. The problem with this approach is that there is a lack of transparency about data taken from databases. For example suppose a neuroscientist collected some fMRI (functional MRI; brain imaging) data. It is likely that there were a number of pre-processing steps that were undertaken throughout this data collection procedure. For instance, not every brain is alike, and the images need to be co-registered to an anatomical space and then spatially-normalized (squashed and stretched to match some image template). The images are also blurred to accommodate the heterogeneity in individual anatomy. Finally the images are then typically converted to a matrix of numbers. Clearly the process of translating the original image to numeric data involved a large number of assumptions. If a statistician simply obtained their fMRI data from a database, they would be ignorant to the assumptions made and the pre-processing steps taken, each of which are significant sources of noise in the data. All they see is a matrix of numbers (or many matrices of numbers) but they do not know how those numbers arose from the experiment was undertaken which leads to the possibility of misinterpretation.</p>

<p>As exemplified above, it is extremely important to understand the experimental design including all pre-processing steps when analyzing data. It is vital to understand what the numbers in your dataset mean and how they were obtained. Moreover, if the experimental design was poor, then the statistician will be able to take this knowledge into consideration for their analysis and will know to approach with caution and take the conclusions generated with a grain of salt. Numbers are not magical; they can be severely misleading if their origin is ignored. The upshot of this discussion is that we should be weary of analyses performed on data taken from sources without knowledge of the data collection process.</p>

<h2>Why should we shift away from &quot;optimality&quot;?</h2>

<p>It is an unfortunate truth that statistics has become a field obsessed with optimality. Although this may seem like a positive direction in theoretical statistics (why <em>wouldn&#39;t</em> we want to find the method with the lowest risk or the quickest run-time?), in the world of applied statistics this push for optimality is resulting in a statistical philosophy that searches for the &quot;biggest&quot;, &quot;best&quot; and &quot;fastest&quot; method, forgoing interpretability. Moreover, in the vast majority of situations in applied statistics there is no single optimal approach. By requiring optimality, even by defining optimality, we are constraining our freedom to solve the problem at hand. Firstly, how would one even define optimality in terms of an approach to an applied problem? It&#39;s not hard to identify good and bad approaches, but how can we specify that one single approach is &quot;optimal&quot;? In my experience, I have never seen a data problem for which I feel I have found the &quot;optimal&quot; approach. </p>

<p>Even in most realms of theoretical statistical optimization (perhaps with the exception of convex optimization), quite often the result is not a globally optimal approach, but rather a local one.</p>

<h2>Critical thinking</h2>

<p>The need to move away from the search for an optimal solution may understandably leave you asking what type of solutions you should seek. This is where judgement comes in: it may seem contradictory to many of you with a traditional objective statistical education, but in reality many of the decisions made in applied statistical projects are brought about by thinking critically about the problem and making judgement calls.</p>

<p>Many, if not most, applied statistical problems you will face in reality cannot necessarily be solved using straightforward traditional method. As an applied statistician, you will be faced with non-conventional problems which will require you to think critically and creatively, rather than automatically. Since no two problems are the same, the kind of skills required for true applied statistics will, for most people, be much harder to learn than formulaic mathematical skills. These skills are significantly more subjective and non-linear; traits that make many people uncomfortable (particularly those who would much rather prove theorems).</p>

<p>Although this book focuses on taking a step back from the theory when tackling applied problems, it certainly not true that theory is useless! Far from it; good theorems are extremely useful, and are indepsensible to the development and advancement of statistics. In the modern academic realm, however, there is such an expectation to produce new theorems, that a large amount of theorems presented in papers today encompass a disconnect between the assumptions upon which they are based and the methods.</p>

<h2>Using statistics in real life</h2>

<p>We have now arrived at the point where we talk about the increasing disconnect between applying statistics in real life and learning (in a class, for example) about statistical methods. This section is in essence a justification for the entire existence of this book. </p>

<p>If you&#39;ve ever been in an applied statistics class, the chances are high that it went something like this:</p>

<ol>
<li><p>&quot;Here&#39;s a new method! Let&#39;s learn how to derive it!&quot;</p></li>
<li><p>&quot;It&#39;s homework time! Let&#39;s use this specific method we just learned how to derive on this pretty, clean and uninteresting dataset and report a p-value!&quot;</p></li>
<li><p>Repeat steps 1-2 with new methods until the semester ends at which point you will be tested on whether you know how to derive the specific method and use each method when prompted on simple examples.</p></li>
</ol>

<p>If this was not your experience, congratulations; perhaps you learned something useful. The problem with the disturbingly common approach described above (ignoring, for the moment, issues with p-values) is that it bears virtually no connection to what it is really like to analyze data (also this approach can be really, really boring). Firstly, in the real world, there is typically no one ti tell you what method to use and when. The general idea is to use what you know and then expand your search for a solution and learn new approaches. It is important to realize that within most classroom settings, you are deliberately given clean, doable problems. In reality, the data you encounter will most likely be messy, confusing and often the questions you ask may be impossible to answer using the data at hand. Statisticians need to be comfortable exploring data without guidelines; they need to gain &quot;data wisdom&quot;.</p>

<h3>Data wisdom</h3>

<p>Data wisdom is &quot;the ability to combine domain, mathematical and methodological knowledge with experience, understanding, common sense, insight and good judgement in order to think critically about data and to make decisions based on data&quot;. The essence of data wisdom can be encompassed in the following set of (not necessarily sequential) questions that a data analyst is encouraged to ask before embarking on and during any data analysis project.</p>

<h4>1. Question</h4>

<p>It is important to explicitly formulate questions to ask. For example, in neuroscience: how does the brain work? Or in banking: to which group of customers should a bank promote a new service? This is where the domain experts come in. They will know which questions are important and how to properly formulate them. Interaction with domain experts is absolutely necessary for the success of the data science project to come.</p>

<h4>2. Collection</h4>

<p>What are the most relevant data to collect to answer the question we are interested in exploring? If the analyst did not collect the data themselves, they should be asking how, when and where the data were collected and what the data is measuring. More explicitly, who collected the data? Over what time period? At what locations? What instruments were used?</p>

<h4>3. Meaning</h4>

<p>Not only must we understand where the data came from, but also what it really measures. For example, it is surprisingly important to ask questions such as what does a number mean in the data? Does it measure what it was supposed to measure? How could things go wrong? What statistical assumptions is one making by assuming that things didn&#39;t go wrong?</p>

<h4>4. Relevance</h4>

<p>Is the data collected relevant to answering the question formulated at the birth of the project? If not, what other data should one collect?</p>

<h4>5. Translation</h4>

<p>The analyst must be able to translate the question being asked into a statistical setting. For example, if there are multiple possible translations (such as a translation into a prediction problem or a translation into an inference problem regarding a statistical model), which is the most appropriate? </p>

<h4>6. Comparability</h4>

<p>The analyst should assess the ability of the data to satisfy the assumptions made by the translation chosen in the previous step. For example, are the data units are comparable (you don&#39;t want to combine apples and oranges)? Do they need to be normalized? </p>

<h4>7. Visualization</h4>

<p>Visualization is an incredibly useful tool for assessing the suitability of both the data and statistical translation in answering the question at hand. Assessing plots of the data can reveal patterns, inconsistencies and unexpected data points that may require further examination. </p>

<h4>8. Randomness</h4>

<p>It is extremely important to identify where the randomness in the data comes from (for example, was it from the sampling procedure or a random treatment assignment mechanism?). The source of randomness in the data must correspond to the randomness assumed by the statistical model. </p>

<h4>9. Stability</h4>

<p>The next step is to explore the validity of the conclusions drawn. For example, do different methods yield the same qualitative conclusions? Do perturbations (perhaps by adding noise or subsampling exchangeable data units) change the conclusions drawn? Such checks are vital in ensuring reproducibility. </p>

<h4>10. Validation</h4>

<p>Finally, the conclusions should be explicitly tested through validation methods, such as by testing the conclusions on other kinds of data.</p>

<p>The concept of data wisdom is based on the notion of <em>uncontrolled inspiration</em>: the approach by which we solve problems by first attempting a solution, then, if this solution is unsatisfactory, spending some time reading (or even better, talking to people) the relevant domain and statistical literature to draw inspiration for a new or modified approach, and repeating this process until we have a &quot;satisfactory&quot; solution (keeping in mind that there may be no such thing). Statisticians must be able to learn as they go along.</p>

<p>In this way, data analysis is like detective work: we first form a hypothesis and we then look for evidence in the data to support this hypothesis. Data analysis is also like peeling an onion. You peel one layer, and then another layer, and so on and so forth until you find what you&#39;re looking for. The only real difference between statistical data analysis and onions in that onions are guaranteed to have a finite number of layers.</p>

<p><FONT COLOR="red">Above are three nice analogies (uncontrolled inspiration, detective, onion) -- probably choose one<FONT></p>

</section>

<footer>

<p><small>Site powered by <a href="http://jekyllrb.com/">Jekyll, <a href="http://pandoc.org/">pandoc and <a href="http://yihui.name/knitr/">knitr</a>.</small></p>
</footer>


</div>
<script src="javascripts/scale.fix.js"></script>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-39541792-2', 'github.com');
ga('send', 'pageview');
</script>    
</body>
</html>
<!doctype html>
<html>
<head>

<!-- LaTeX -->

<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<title>Statistical Models: Theory and Applications</title>

<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<!--	<link href='http://fonts.googleapis.com/css?family=Fanwood+Text:400,400italic' rel='stylesheet' type='text/css'> -->
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<!--[if lt IE 9]>
 <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
 <![endif]-->
</head>
<body>
<div class="wrapper">
<header>
<h1><a href="index.html">Statistical Models: Theory and Applications</a> <h1><a><medium>Bin Yu and Rebecca Barter</medium></a></h1></h1>
<p></p>

<p class="view"><a href="https://github.com/rlbarter/Stat_Models_Book">View the Project on GitHub <small>rlbarter/Stat_Models_Book</small></a></p>




</header>










<section>  



<h1>The bias-variance tradeoff</h1>

<p>Throughout our data analysis adventures, it is vital to identify how various sources of error we encounter might lead to &quot;incorrect&quot; or &quot;unreliable&quot; estimates. Such knowledge would allow us to improve the models we fit by manipulating the tradeoff between bias (how close our estimate is on average to the true value) and variance (how much our estimate varies over repeated samplings).</p>

<p>In essence, the bias-variance tradeoff captures the tradeoff between accuracy and reliability of our estimates. </p>

<h5>Bias</h5>

<p>Before describing the bias-variance tradeoff, we will first delve into a discussion of bias itself. For a technical definition, the bias of an estimator $T(X)$ (where $T(X)$ is simply a statistic: a function of our data such as the sample mean $\bar{X}$) of a parameter $\theta$ is defined to be </p>

<p>$$\text{Bias} = E(T(X)) - \theta$$</p>

<p>If $T(X)$ is an unbiased estimator of $\theta$, then $E(T(X)) = \theta$: on average (assuming we could take repeated samples from our population and recalculate our estimate), the value of our estimator is equal to the <em>true</em> value of the parameter.</p>

<p>Suppose that we&#39;ve just obtained some data $x_1, ..., x_n$, let&#39;s say the heights of $n$ randomly selected people. Suppose that we plot a histogram the $n$ data points and conclude that the histogram looks kind of Gaussian (obviously we are only observing a sample so we will never have <em>exactly</em> normal data). Let&#39;s think for a moment about the assumption that the true distribution of the data is actually a normal distribution with mean $\mu$ and variance $\sigma^2$. Do you really believe that the data that you observe was exactly generated by this distribution? Probably not, right? The <strong>true</strong> underlying distribtion of the data, in general, cannot be <em>exactly</em> specified. If we assume that the data comes from a specific space of probability distributions (such as the space of normal distributions indexed by $\mu$ and $\sigma$), then the <em>bias</em> of this assumption/estimation is the &quot;distance&quot; (in some sense) between the distribution that we assumed and the &quot;true&quot; distribution. An example of the distance between the true (unknown) distribution, $P^*$, and our assumed distribution, $P_\theta$, is the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a> (a distance metric very prominent in information theory):</p>

<p>$$D_{KL}(P^* \vert \vert P_\theta) = \int_{-\infty}^\infty p^*(x) \ln \frac{p^*(x)}{p_{\theta}(x)} dx$$</p>

<p>where $p^*$ and $p_\theta$ denote the densities of $P^*$ and $P_\theta$. It is not hard to see that minimizing this quantitiy is actually the same as finding the $\theta$ that maximizes likelihood:</p>

<p>$$
\begin{aligned}
D_{KL}(P^* \vert \vert P_\theta) &amp; = \int p^*(x) \ln p^*(x) dx - \int p^*(x) \ln p_\theta(x) dx\\
&amp; = - H\left(p^*(x)\right) - \int p^*(x) \ln p_\theta(x) dx\\
&amp; = - H\left(p^*(x)\right) - E\left(\ln p_\theta(x) \right)
\end{aligned}
$$</p>

<p>Where $H\left(p^*(x)\right)$ is something called the <em>entropy</em> of $p^*(x)$, which is independent of $\theta$. Thus, if we want to find the best $P_{\theta}$, we want to minimize the KL-divergence with respect to $\theta$, which is equivalent to maximizing the expected log-likelihood, $E(\ln p_\theta(x))$ (or minimizing the negative expected log-likelihood).</p>

<p>Thus it is important to be aware that the parametric bias that we see in text books essentially assumes that the distirbution we assume is the true distribution. It is important to think about the context! </p>

<p>Most of the statistical literature lives within the world that assumes specific distributions in asymptotic settings. However, in reality, we will never truly be able to reach the land of asymptotics. Thus, when applying theoretical results to real data, it is important to take the conclusions with a grain of salt: the assumptions behind the results are almost never more than approximately satisfied. The mathematics are only relevent when your approxiamtion is good enough relative to the precision you&#39;re looking for. We should worry about how accurate the assumptions we&#39;re making are, and if they are not valid, how does this affect the conclusions we draw?</p>

<p>In fact, talking about the existence of an underlying truth in any setting is already a fairly hefty assumption. </p>

<p>Where does bias come from? Bias can arise from various sources, such as our sampling procedure (if we are interested in estimating the average height of members of a population, but we accidentally sample many more men than women, then we are getting an upward biased estimate of height for the entire population), as well as the model (estimator) we use.</p>

<h5>Variance</h5>

<p>Suppose that we were interested in measuring the average height of all students enrolled at UC Berkeley (this is our population of interest). Assuming that we have only one hour to do so, we cannot possibly measure the height of every enrolled student, since many will not be at the campus during that hour. Thus we must suffice to estimate the average height of all students using a random sample of students drawn from those on campus during the given hour. From this sample of students, we calculate their average height, and assuming that this sample of students can be thought of as &quot;randomly selected&quot; from the overall student population, this height estimate will be an unbiased estimate of the average height of the entire student population. However, we have only observed a single number (the average height of all students on campus during the hour). How can we get a sense of how much this estimate would change for a different sample of studnets? </p>

<p>It is important to realize that this estimate of average height is in fact a <em>random variable</em>. But what is random about it? Where did the randomness come from? The randomness came from the sampling procedure. We drew a random sample of the students on campus during the hour of interest, and in turn our estimate of the average height of the student population is a random number: it would have a different value for a different sample. The question is, how different might the average height be for another random sample? This question encapsulates the idea of variability. How much would our estimate vary from the mean value if we drew other samples from the population? Notationally, we typically write the variance of a random variable $X$ as</p>

<p>$$\text{Var}(X) = E \left( X - E(X) \right)^2$$</p>

<p>This entity represents, on average, how much a random variable deviates from its mean. Note that if we were considering the variance of an estimator, then the variance of the estimator describes, on average, how much the estimator (a random variable) deviates from its mean <em>not</em> how much it deviates from the &quot;true&quot; value of the parameter (unless the estimator is unbiased).</p>

<p>If we have a sample of independent and identically distributed (IID) random variables, $X = (X_1, ..., X_n)$ (this could be a sample of estimates, rather than a sample of observations), then we can estimate the true variance of $X$by the sample variance given by</p>

<p>$$\widehat{\text{Var}(X)} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2$$</p>

<p>where we divide by $n-1$, rather than $n$, so that this estimator of the variance is unbiased. This result is shown in the Appendix, but you&#39;re encouraged to show that this is an unbiased estimator for the true variance of the population.</p>

<p>If we&#39;re willing to assume that the data comes from a normal distribution (which, for height is probably not a terrible assumption), then there exist asymptotic results that tell us the variance of our estimate when our sample size is extremely large. However, what if we didn&#39;t feel comfortable assuming normality? We could use bootstrapping (of which there exist several versions)....</p>

<p>When we estimate features of the population (such as model parameters) using our observed data, we are observing</p>

<h5>Questions to ask yourself:</h5>

<ol>
<li>Describe situations where the asymptotic where the asymptotic bias and variance approximations are relevant to data analysis, as well as situations where they are not. How might we find out if the asymptotic results are relevant to the problem at hand?</li>
</ol>

<h5>Manipulating the tradeoff: regularization</h5>

<p>We mentioned above that short bandwidths correspond to an estimator, $\hat{f}_{n, h}(x)$, of $f$ that has low bias, but high variance, whereas long bandwidths corresponds to an estimator that has high bias but low variance. Is one of these two extremes better than the other? Is it possible to obtain a happy midground between the two? This is where the bias/variance trade-off comes in: perhaps we would rather find an estimator that minimizes a combination of the bias and the variance, such as the mean squared error (MSE):</p>

<p>$$\text{MSE} = \text{Bias}^2 + \text{Variance}$$</p>

<p>In fact, in modern statistics, gains are often obtained by increasing bias, but reducing variance. We can add a regularization (or smoothing) parameter to control the trade-off between bias and variance (we will return to this concept when we discuss the lasso linear model).</p>

<h2>Appendix: the sample variance is unbiased</h2>

<p>We want to first show that the sample variance discussed above is unbiased. Suppose that we draw an IID sample $X_1, ..., X_n$ from a population which has a true variance $\sigma^2$ and true mean $\mu$. Suppose that the values that we observe are given by $x_1, ..., x_n$ (once we actually observe the values, they are no longer random; they aren&#39;t going to suddenly change when we blink! Rather, they are rather realizations of a quantity that was generated by a random process). </p>

<p>We want to show that
$$\sigma^2 = E(\widehat{\text{Var}(X)}) = E\left( \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2\right)$$</p>

<p>By the linearity of expectation, we have that
$$
\begin{aligned}
E(\widehat{\text{Var}(X)}) &amp; = \frac{1}{n-1} \sum_{i=1}^n E \left( X_i - \bar{X} \right)^2\\
&amp; =  \frac{1}{n-1} \sum_{i=1}^n E\left( X_i^2 - 2 \bar{X} X_i + \bar{X}^2 \right)\\
&amp; = \frac{1}{n-1} \sum_{i=1}^n E\left(X_i^2 \right) - 2 E\left(\bar{X} X_i \right) + E\left( \bar{X}^2 \right)\\
&amp; = \frac{1}{n-1} \left[\sum_{i=1}^n E\left(X_i^2 \right) - 2 n E\left(\bar{X}^2 \right) + nE\left( \bar{X}^2 \right)\right]\\
&amp; = \frac{1}{n-1} \left[\sum_{i=1}^n E\left(X_i^2 \right) - nE\left( \bar{X}^2 \right)\right]\\
\end{aligned}
$$</p>

<p>Now, since we can write the variance of $X$ with variance $\sigma^2$ and mean $\mu$ (in which case the variance of $\bar{X} = \frac{\sigma^2}{n}$) as 
$$\sigma^2 = \text{Var}(X) = E\left(X_i^2\right) - \left( E\left( X_i\right)\right)^2 =  E\left(X_i^2\right)  - \mu^2$$</p>

<p>we have that</p>

<p>$$
\begin{aligned}
E(\widehat{\text{Var}(X)}) &amp; = \frac{1}{n-1} \left[\sum_{i=1}^n (\sigma^2 + \mu^2) -  n \left(\frac{\sigma^2}{n} + \mu^2 \right) \right]\\
&amp; = \frac{1}{n-1} \left[ n(\sigma^2 + \mu^2) -  n \left(\frac{\sigma^2}{n} + \mu^2 \right) \right]\\
&amp; = \frac{1}{n-1} \left[ (n-1)\sigma^2 \right]\\
&amp; = \sigma^2
\end{aligned}
$$</p>

<p>Thus, the given sample variance given above is an unbiased estimate for the population variance, $\sigma^2$.</p>

<h3>Answers to the questions:</h3>

<ol>
<li><strong>Describe situations where the asymptotic where the asymptotic bias and variance approximations are relevant to data analysis, as well as situations where they are not. How might we find out if the asymptotic results are relevant to the problem at hand?</strong>
<p>One example is when the model approximation error dominates the sampling error: if the class of models that you&#39;re trying to fit is extremely far from the truth, then the asymptotic results for the model become meaningless.</p>
<p>This is actually an important question to keep in mind when applying such results: when does it apply? Does my data satisfy the assumptions that make the results valid? You need evidence (although, in reality there are many assumptions that are sadly untestable)</p></li>
</ol>



</section>

<footer>

<p><small>Site powered by <a href="http://jekyllrb.com/">Jekyll, <a href="http://pandoc.org/">pandoc and <a href="http://yihui.name/knitr/">knitr</a>.</small></p>
</footer>


</div>
<script src="javascripts/scale.fix.js"></script>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-39541792-2', 'github.com');
ga('send', 'pageview');
</script>    
</body>
</html>
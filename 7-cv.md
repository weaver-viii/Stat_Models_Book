---
title: "Cross validation"
author: "Rebecca Barter"
output: html_document
---

# Cross validation 



Cross validation (CV) is an extremely useful technique initially developed for use in model validation, but with extremely broad uses. CV is a generalization of the idea of witholding a test set from analysis for the purpose of validating results using independent data. The idea is, after we've fit a model, it would be really nice if we could test it on new data. Let's consider an example of using CV to assess prediction performance.

1. Divide the samples into $K$ equal (or as equal as possible) cross-validation folds (subsets of the data) at random.

2. For each fold, $k = 1, 2, ..., K$, the set of observations in the $k$th fold is called the *test set*, while the remaining data is called the *training set*.

    a) Perform the analysis or fit the model using only the observations in the training set.
      
    b) Validate the analysis or assess the model fit (e.g. by calcualting prediction error) using the observations from the witheld test set.
  
3. Calculate the average or sum of the values calculated (such as the prediction errors) over each test set fold.



This procedure provides the user with an empirical estimate of their prediction error.



One problem with the use of cross-validation is that it is often misinterpreted as the *true* prediction error. In fact, in most situations, we can never know the true prediction error (if we could, then why would we need to estimate it using cross-validation?).


As a general rule of thumb, instead of relying on cross-validation with the data you've used to define your models and to perform your analysis (although this is a reasonable thing to do, particularly for medium-sized datasets), as soon as you recieve the data, you should immediately put some subset aside and not touch it until you've truly finished with your analysis. This subset will be your validation set.


As another thing to keep in mind, in general, we have no reliable way to put "error bars" on prediction errors generated by cross-validation.

Cross-validation can have huge error.... <FONT COLOR="red">show evidence</FONT>
